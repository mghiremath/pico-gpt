{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "047851f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as input.txt ✅\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "print(\"Saved as input.txt ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "451aed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f: # Read the file content to inspect it\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ec91a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\" Length of dataset in characters:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfe29f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print first 1000 characters to verify\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a60ef14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      " All the unique characters in the dataset: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# store all the unique characters in a set\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(\" All the unique characters in the dataset:\", ''.join(chars))\n",
    "print (vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7dc9bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 'hello': [46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder function: takes a string and returns a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder function: takes a list of integers and returns a string\n",
    "print(\"Encoding 'hello':\", encode(\"hello\"))\n",
    "print(decode(encode(\"hello\")))  # should return 'hello'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d467a36",
   "metadata": {},
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(\"Encoding 'hello' with tiktoken:\", enc.encode(\"hello\"))\n",
    "\n",
    "print(\"Decoding tiktoken encoding:\", enc.decode(enc.encode(\"hello\")))  # should return 'hello'\n",
    "\n",
    "enc.n_vocab  # number of tokens in the vocabulary\n",
    "\n",
    "print(\"Number of tokens in the vocabulary:\", enc.n_vocab)\n",
    "\n",
    "enc.encode(\"hello\")\n",
    "\n",
    "enc.decode(enc.encode(\"hello\"))  # should return 'hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f8cdf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded data shape and datatype: torch.Size([1115394])\n",
      "First 1000 characters of encoded data: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n",
      "Decoded first 1000 characters: First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now let's encode the entire text dataset and store it in a torch.Tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long) # convert the list of integers to a torch.Tensor\n",
    "print(\"Encoded data shape and datatype:\", data.shape)\n",
    "print(\"First 1000 characters of encoded data:\", data[:1000]) # the 1000 characters we looked at earlier will look to the GPT model like this\n",
    "print(\"Decoded first 1000 characters:\", decode(data[:1000].tolist()))  # convert the first 1000 integers back to characters to verify correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc6517d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([1003854])\n",
      "Validation data shape: torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "# Let's split the data into train and validation sets\n",
    "n = int(0.9*len(data)) # 90% for training, 10% for validation\n",
    "train_data = data[:n]  # to train\n",
    "val_data = data[n:] # to validate and to check if the model is overfitting, we do not want model to memorize the training data\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Validation data shape:\", val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "643e8e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 # how many characters to predict at once\n",
    "train_data[:block_size+1]  # the first 9 characters of the training data\n",
    "# we will use this to train the model, the first 8 characters will be the input\n",
    "# and the 9th character will be the target output\n",
    "# for example, if the first 9 characters are \"hello worl\", we will use\n",
    "# \"hello worl\" as input and \"hello world\" as target output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7909c5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x= train_data[:block_size]  # the first 8 characters\n",
    "y = train_data[1:block_size+1]  # the next 8 characters, which is the target output\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11a495cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([4, 8])\n",
      "Target batch shape: torch.Size([4, 8])\n",
      "Input batch: tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Target batch: tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Batch 0, when input is [24] the target: 43\n",
      "Batch 0, when input is [24, 43] the target: 58\n",
      "Batch 0, when input is [24, 43, 58] the target: 5\n",
      "Batch 0, when input is [24, 43, 58, 5] the target: 57\n",
      "Batch 0, when input is [24, 43, 58, 5, 57] the target: 1\n",
      "Batch 0, when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "Batch 0, when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "Batch 0, when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "Batch 1, when input is [44] the target: 53\n",
      "Batch 1, when input is [44, 53] the target: 56\n",
      "Batch 1, when input is [44, 53, 56] the target: 1\n",
      "Batch 1, when input is [44, 53, 56, 1] the target: 58\n",
      "Batch 1, when input is [44, 53, 56, 1, 58] the target: 46\n",
      "Batch 1, when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "Batch 1, when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "Batch 1, when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "Batch 2, when input is [52] the target: 58\n",
      "Batch 2, when input is [52, 58] the target: 1\n",
      "Batch 2, when input is [52, 58, 1] the target: 58\n",
      "Batch 2, when input is [52, 58, 1, 58] the target: 46\n",
      "Batch 2, when input is [52, 58, 1, 58, 46] the target: 39\n",
      "Batch 2, when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "Batch 2, when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "Batch 2, when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "Batch 3, when input is [25] the target: 17\n",
      "Batch 3, when input is [25, 17] the target: 27\n",
      "Batch 3, when input is [25, 17, 27] the target: 10\n",
      "Batch 3, when input is [25, 17, 27, 10] the target: 0\n",
      "Batch 3, when input is [25, 17, 27, 10, 0] the target: 21\n",
      "Batch 3, when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "Batch 3, when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "Batch 3, when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "# Batch dimensions\n",
    "torch.manual_seed(1337)  # for reproducibility\n",
    "batch_size = 4  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # what is the maximum context length for predictions?\n",
    "# Each sequence will be of length block_size, and we will have batch_size sequences in parallel\n",
    "# For example, if we have a sequence of length 32, we can create 4 sequences of length 8\n",
    "# The first sequence will be the first 8 characters, the second sequence will be the next 8 characters, and so on\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # random starting indices for each sequence in the batch\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])  # input sequences\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # target sequences, shifted by one character\n",
    "    return x, y\n",
    "\n",
    "# Example usage\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Input batch shape:\", xb.shape)  # should be (batch_size, block_size\n",
    "print(\"Target batch shape:\", yb.shape)  # should be (batch_size, block_size)\n",
    "print(\"Input batch:\", xb)  # the input sequences\n",
    "print(\"Target batch:\", yb)  # the target sequences, shifted by one character\n",
    "\n",
    "print('--'*50)\n",
    "\n",
    "for b in range(batch_size): # for each sequence in the batch\n",
    "    for t in range(block_size):  # for each character in the sequence\n",
    "        context = xb[b, :t+1]  # input sequence up to the t-th character\n",
    "        target = yb[b, t]  # target character at position t\n",
    "        print(f\"Batch {b}, when input is {context.tolist()} the target: {target.item()}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b74295b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "4.878634929656982\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):  # inherits from nn.Module\n",
    "    def __init__(self, vocab_size):  # vocab_size is the number of unique characters in the dataset\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None): # idx is the input sequence, targets is the target sequence\n",
    "        logits = self.token_embedding_table(idx)  # get the logits for the next token\n",
    "        if targets is not None:  # if we have targets, we compute the loss\n",
    "            # reshape the logits to (batch_size, block_size, vocab_size) - (B, T, C)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # reshape to (batch_size * block_size, vocab_size)\n",
    "            targets = targets.view(B * T) # reshape targets to (batch_size * block_size)\n",
    "            loss = F.cross_entropy(logits, targets) # compute the loss(cross entropy) between the logits and the targets\n",
    "            # logits shape: (batch_size, block_size, vocab_size)\n",
    "        else:\n",
    "            loss = None  # if we do not have targets, we do not compute the loss\n",
    "        return logits, loss  # return the logits and the loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):  # idx is the input sequence, max_new_tokens is the number of tokens to generate\n",
    "        for _ in range(max_new_tokens):  # generate max_new_tokens tokens\n",
    "            logits, loss = self(idx)  # forward pass, we do not need targets for generation\n",
    "            logits = logits[:, -1, :]  # take the logits for the last character in the sequence (B, C)\n",
    "            probs = F.softmax(logits, dim=-1)  # convert logits to probabilities (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # sample from the probabilities (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # append the sampled token to the input sequence (B, T+1) \n",
    "        return idx  # return the generated sequence\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)  # forward pass\n",
    "print(logits.shape)  # should be (batch_size, block_size, vocab_size)\n",
    "print(loss.item())  # print the loss value\n",
    "#idx = torch.zeros((1, 1), dtype=torch.long)  # start with a single token (the first character)\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))  # generate 100 new tokens and decode them to a string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d54a4",
   "metadata": {},
   "source": [
    "the generated output is garbage as we didn't train it using history, which we will do later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91808a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)  # AdamW optimizer with a learning rate of 1e-3 ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4689e69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.311746120452881\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # how many independent sequences will we process in parallel?\n",
    "for steps in range(10000):  # train for 100 steps\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')  # get a batch of data\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)  # forward pass\n",
    "    optimizer.zero_grad(set_to_none=True)  # zero the gradients\n",
    "    loss.backward()  # backpropagation\n",
    "    optimizer.step()  # update the parameters\n",
    "print(loss.item())  # print the loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39ba963f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inghibey?\n",
      "\n",
      "Wher t Moman llou f thar werd felld:\n",
      "CIns,\n",
      "Shtonotre de.\n",
      "Nureendyoous:\n",
      "PULAng, t? yothe'sefofad y hapareove.\n",
      "CENI t f s fy d l the, ys.\n",
      "\n",
      "TI,\n",
      "An ceatmisce jesshe ee.\n",
      "CH: t thed tewenoncrgis an mp?\n",
      "Bunevest-\n",
      "Bu oucy,\n",
      "Hen latas t\n",
      "LYoflin t o ldgur wey;\n",
      "Air? LIO, nderis, wemysoncer INI wir oul thalivoccouy d,\n",
      "\n",
      "\n",
      "MI myomave I nd-then, d whanghe ther am m.\n",
      "Toupove y\n",
      "Wavie athe he indr:\n",
      "\n",
      "and\n",
      "Pracor jer leathewomicefownge 'shyouclen he t, feret, bind ig qu;\n",
      "Coy IS:\n",
      "Yolllet juns w t y\n",
      "Fonowangonesthed cealou none sur ply,\n",
      "Nombu al?\n",
      "\n",
      "And nom ma,\n",
      "SBuburou'l bital ts theendist, wo Be; alllo.\n",
      "sadacew m yos o f gelllooter, wbene d y s. mounthigrave he hal irbe, t?\n",
      "HONu as, as os?\n",
      "Thirsin porey d\n",
      "TI age\n",
      "I s yeryouresthovo atir s,\n",
      "\n",
      "Pi's t f tr, du?\n",
      "EDUSobeeathichemeg g y toul IULAlmithu ke, w bhe ne uen thery RENToon, IZEEDUCI cend ot l me's angice;\n",
      "Thewareratulyendutonde 's thie,\n",
      "fr arnd ic-mybem t m'dsent ee, blouged fere fearau se fofouriey'd!\n",
      "LANEThellld ogo withend tequ\n",
      "Y ldill, totooou\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=1000)[0].tolist()))  # generate 100 new tokens and decode them to a string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82517878",
   "metadata": {},
   "source": [
    "Here, we can see, after training for rougly a million iterations, it sort of looks like shakespeare but definitely not it. \n",
    "We are only seeing the last character to predict the next one, but now we will try to make the characters to start to talk to each otehr and figuring what is the context so that they can predict what comes nect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c23cf06",
   "metadata": {},
   "source": [
    "# The mathematical trick in self-attention\n",
    "\n",
    "The tokesn shouldn't talk to future tokens, but only previous ones. the information should donly flow from previous contexts to the current timestep and predict the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b43407c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following example\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2  # batch size, sequence length, number of channels (vocab size)\n",
    "x = torch.randn(B, T, C)  # random input tensor\n",
    "x.shape  # should be (4, 8, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad77eac",
   "metadata": {},
   "source": [
    "# Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2cf7862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]]) \n",
      " tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "# we want  x[b,t]  = mean_{i<=t} x[b,i]  (the mean of all the previous time steps including the current one)\n",
    "# we can do this using a for loop, but it is least efficient\n",
    "xbow = torch.zeros((B, T, C))  # initialize the output tensor bag of words\n",
    "for b in range(B):  # for each sequence in the batch\n",
    "    for t in range(T):  # for each time step in the sequence\n",
    "        xprev = x[b, :t+1]  # all previous time steps including the current one (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)  # mean over the time dimension\n",
    "print(x[0],'\\n', xbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0108d2",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302ec5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b:\n",
      " tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c:\n",
      " tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a= a / torch.sum(a, 1, keepdim=True)  # normalize the rows- sum of each row is 1\n",
    "b = torch.randint(0,10, (3, 2)).float()  # random tensor with values between 0 and 10, shape (3, 2)\n",
    "c = a@b  # matrix multiplication\n",
    "print(\"a:\\n\", a)\n",
    "print(\"b:\\n\", b)\n",
    "print(\"c:\\n\", c)  # should be a matrix of shape (3, 2)\n",
    "# here, the rows are multiplied by the corresponding rows of b and summed up, but since a is lower triangular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5320a385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(T, T))  # lower triangular matrix of shape (T, T)\n",
    "weights = weights / weights.sum(1, keepdim=True)  # normalize the rows\n",
    "weights\n",
    "xbow2 = weights @ x # this is a batch matrix multiplication in parallel (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "torch.allclose(xbow[0], xbow2[0]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8430e1b6",
   "metadata": {},
   "source": [
    "# Version 3 (Use softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9da36ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.zeros((T, T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # for all tril elements that are zer0, make them -inf\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a35a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.zeros((T, T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # for all tril elements that are zero, make them -inf\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a821b241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = torch.zeros((T, T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim = -1)\n",
    "xbow3 = weights @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1d24a",
   "metadata": {},
   "source": [
    "🔤 vocab_size\n",
    "\n",
    "The total number of unique tokens (characters or words) in your dataset.\n",
    "\n",
    "You use vocab_size:\n",
    "- In nn.Embedding(vocab_size, ...): Because each token index must map to a unique embedding vector.\n",
    "- In nn.Linear(..., vocab_size): Because your model must output a probability distribution over all possible next tokens.\n",
    "\n",
    "👉 Think:\n",
    "\n",
    "“How many different outputs do I have to predict from?” — That’s your vocab_size.\n",
    "\n",
    "⸻\n",
    "\n",
    "📐 n_embd (embedding dimension)\n",
    "\n",
    "How many features you want for each token or position embedding.\n",
    "\n",
    "You use n_embd:\n",
    "- In nn.Embedding(vocab_size, n_embd): to embed each token as a vector of size n_embd.\n",
    "- In nn.Embedding(block_size, n_embd): to embed each position as a vector of size n_embd.\n",
    "- In nn.Linear(n_embd, vocab_size): to transform the internal representation into logits over the vocabulary.\n",
    "\n",
    "👉 Think:\n",
    "\n",
    "“How big is the vector I use to represent each token and position?” — That’s your n_embd.\n",
    "\n",
    "⸻\n",
    "\n",
    "🔢 block_size\n",
    "\n",
    "The maximum context length the model can see at once (how many tokens in one input sequence).\n",
    "\n",
    "You use block_size:\n",
    "- In nn.Embedding(block_size, n_embd): Because each position from 0 to block_size - 1 needs its own embedding.\n",
    "- For slicing input text into fixed-size chunks of length block_size.\n",
    "\n",
    "👉 Think:\n",
    "\n",
    "“How far back can the model look when predicting the next token?” — That’s your block_size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fc3ac1",
   "metadata": {},
   "source": [
    "# Version 4: self-attention!!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cf467ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32   # B=batch size, T=sequence length (block size), C=embedding dim(channels)\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16       # Number of output channels for each attention head\n",
    "key = nn.Linear(C, head_size, bias = False)\n",
    "query = nn.Linear(C, head_size, bias = False)\n",
    "value = nn.Linear(C, head_size, bias = False)\n",
    "\n",
    "# Every token in (B, T) arrangement produce 2 vectors independently - query(what am i looking for) and key(what do i contain) - no communications yet\n",
    "# value is an element or vector that is used to aggregate instead of raw 'x'. It represents what the token provide as an output.\n",
    "# By forwarding x into these modules, we will get:\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "v = value(x) \n",
    "\n",
    "# all the queries will dot product with all the keys.\n",
    "\n",
    "weights = q @ k.transpose(-2,-1)   # (B, T, 16) @ (B, 16, T) ---> (B, T, T) for every row of B, we have a T*T matrix giving us affinities\n",
    "# This gives the attention score between each pair of tokens in the sequence: For each pair of token positions (i, j), how much token i should attend to token j.\n",
    "\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) # lower triangular matrix of shape (T, T)\n",
    "#weights = torch.zeros((T, T))\n",
    "weights = weights.masked_fill(tril==0, float('-inf')) # for all tril elements that are zero, make them -inf (This prevents the model from “cheating” by looking at future tokens when predicting the current one — making it causal.)\n",
    "weights = F.softmax(weights, dim=-1) # Each row in wei[b][i] becomes a probability distribution over the previous tokens j <= i\n",
    "#out = weights @ x\n",
    "out = weights @ v #Each token gets a weighted sum of the value vectors of all previous tokens — based on how much it “attends” to each.\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "783a1609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b3706",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
